# Supervised learning
*Assignment #1 - CS 7641 Machine Learning course - Charles Isbell & Michael Littman - Georgia Tech*

Please clone this git to a local project if you want to replicate the experiments reported in the assignment paper.

Virtual Environment
----
This project contains a virtual environment folder [```venv```](placeholder). This folder contains all the files needed to create a virtual environment in which the project is supposed to run. Because of the very large number of files inside of it we are forced to compress the folder into a .zip file in order to upload it to Github. Simply download it and unzip it if necessary, please.

requirements.txt
----
This file contains all the necessary packages for this project. (Running ```pip install -r requirements.txt``` will install all the packages in your project's environment - should not be necessary if you are using the given ```venv```folder here)

The datasets
----
These datasets (```train_32x32.mat```and ```tumor_classification_data.csv```) are the datasets described in the assignment paper. They can be downloaded from their original sources:
* http://ufldl.stanford.edu/housenumbers/train_32x32.mat
* https://www.kaggle.com/uciml/breast-cancer-wisconsin-data

digit_recognition.py and tumor_classification.py
----
These Python files are the implementations of the 5 Machine Learning algorithms studied in this assignment over the two datasets that we use for this project. They are almost identical but the way the data is prepared for the algorithms and the hyperparameters of each algorithm differ from one script to the other because of the differences in the datasets.
What they do is:
- load the dataset
- prepare the data before running it through the algorithms (we use the ```scikit-learn``` algorithms in this project: it is important to follow the requirements of these algorithms for the script to work)
- split the data into training and testing sets
- plot Learning curves on the training and cross-validation sets for each algorithm (generated by splitting the original training set into smaller training and cross-validation sets)
- plot validation curves for each algorithm to test some hyperparameters and get an idea of what range to use in the GridSearch (see next step)
- perform a GridSearch in order to find the best hyperparameters for each algorithm. We perform this GridSearch for the hyperparameters that were tested with the validation curves and over the ranges that were identified as interesting with the help of the validation curves
- train the models with the hyperparameters defined by the GridSearch using the training dataset
- compute the accuracy score of each trained algorithm over the testing dataset (put aside since the beginning of the project) and plot a histogram comparing the differnt accuracy scores
